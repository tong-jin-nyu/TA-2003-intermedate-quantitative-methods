---
title: "APSTA-GE: 2003 Intermediate Quantitative Methods"
subtitle: "Group Exercise Sample Solution"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    df_print: paged
    theme: cosmo
    highlight: haddock
    number_sections: false
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
---

```{r setup, include=FALSE}
# Created on: 10/12/2020
# Modified on: 10/26/2020
# Author: Ying Lu
# Author: Tong Jin

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# Dependencies
library(lm.beta)
library(car)
library(stats)
```

## Description

This file is for the group assignment of **APSTA-GE: 2003 Intermediate Quantitative Methods** course in Fall 2020. It contains sample solutions using base R. 

*All code chunks are heavily scaffolded with detail explanations to show the logic behind solutions.*

### Exercise Questions

[Group Exercise - 2003: IQM - Fall 2020](https://docs.google.com/document/d/1BkZAa453Hmbe4C4ydLiNTpp78Z6HJfA5vXdD-cEQPA4/edit?usp=sharing)

### Data processing

Load `lung_capacity0.csv`, and add a new column labelling the row ID.

**Answer:**

```{r load}
# Load data
# Add a new column: ID: row number
dat <- read.csv("../data/lung_capacity0.csv")
# Create a new column called "ID" and assign the number of rows.
dat$ID <- 1:nrow(dat) # assign 1 to 80 to the "ID" column in `dat`
```

<br>

### Question 1

How does lung capacity change as people get older? (in one sentence)

**Answer:**

```{r Q1}
# Compute the correlation between lung capacity and age
cor(dat$LungCapacitycc, dat$Age)
```

Originally, we thought that lung capacity decreases as people getting older, on average. However, the correlation between lung capacity and age is approximately 0.13. The correlation is not strong enough to observe a clear pattern of association. Therefore, age may not have strong influences on lung capacity.

<br>

#### 1a

Make a scatter plot of lung capacity (on y-axis) and age (on x-axis). 

Eyeball the pattern. What do you see? (in one or two sentences)

**Answer:**

```{r Q1a}
# Create a scatter plot: LungCapacitycc (on y-axis) ~ Age (on x-axis)
plot(LungCapacitycc ~ Age, data = dat)
```

The distribution of lung capacity over age is diverse. We did not observe a clear pattern here.

<br>

#### 1b

Report the sample size.

**Answer:**

```{r Q1b}
nrow(dat)
```

<br>

### Question 2

Conduct a regression analysis to answer the following questions:
	- Which variable is the dependent variable (D.V.)?
	- Which variable is the independent variable (I.V.)?

**Answer:**

```{r Q2}
# Fit a regression model: LungCapacitycc ~ Age
lm_Q2 <- lm(LungCapacitycc ~ Age, data = dat)
summary(lm_Q2)
```

The dependent variable is `lung capacity`. The independent variable is `age`.

<br>

### Question 3

Answer below questions based on the regression result.

#### 3a 

What’s the regression coefficient of `age` on `lung capacity`? 

Report the appropriate regression coefficient and its standard error (S.E.).

**Answer:**

```{r Q3a}
# Get the coefficient part of model summary
summary(lm_Q2)$coefficients
```

The intercept is 5091.225. The standard error is 174.047.

The slope is 4.021. The standard error is 3.532.

The regression equation, in this case, is:

$$
\text{Lung Capacity} = 5091.225 + 4.021 \times \text{Age}
$$

<br>

#### 3b

Is this coefficient statistically significant? What test did you use?

Write down the null and alternative hypotheses.

Report test statistic and p-value.

**Answer:**

We used hypothesis test. 

**Null**: There is no relationship between people's lung capacity and age.

**Alternative:** There exists a relationship between people's lung capacity and age.

```{r Q3b}
# Report the second row of model coefficients
summary(lm_Q2)$coefficients[2, ]
```

This coefficient is not statistically significant because the p-value is greater than 0.05.

<br>

#### 3c

Report a 95% confidence interval for the regression coefficient of age on lung capacity based on the results from the model.

**Answer:**

```{r Q3c}
confint(lm_Q2)
```

The 95% confidence interval for the coefficient of `age` on `lung capacity` is between -3.01 and 11.052.

<br>

### Question 4

Based on the model, calculate the fitted values and residuals for all observations in the model. Answer the following questions based on your results.

#### 4a	

What are the mean values of the fitted values and residuals?

**Answer:**

```{r Q4a}
# Calculate the mean value of fitted values
mean(predict(lm_Q2))
# Calculate the mean value of residuals
mean(residuals(lm_Q2))
```

<br>

#### 4b

Calculate or report the total sum of squares (TSS), model sum of squares (MSS) and residual sum of squares (RSS). 

**Verify that:  TSS = MSS + RSS**

**Answer:**

Option 1: manual calculation

$$
\begin{align}
TSS & = \sum_{i=1}^n (Y_i - \overline{Y})^2 \\
MSS & = \sum_{i=1}^n (\hat{Y_i} - \overline{Y})^2 \\
RSS & = \sum_{i=1}^n (Y_i - \hat{Y_i})^2 \\
\end{align}
$$

```{r Q4b-manual}
# Calculate total sum of squares
TSS <- sum((dat$LungCapacitycc - mean(dat$LungCapacitycc))^2)
# Calculate model sum of squares
MSS <- sum((predict(lm_Q2) - mean(dat$LungCapacitycc))^2)
# Calculate residual sum of squares
RSS <- sum((dat$LungCapacitycc - predict(lm_Q2))^2)

# Check if TSS = MSS + RSS
round(TSS) == round(MSS) + round(RSS)
```

Option 2: use the `anova` function

```{r Q4b}
#use var function to calculate TSS
TSS <- var(dat$LungCapacitycc)*(nrow(dat)-1) 
anova(lm_Q2)
```

Model sum of squares (MSS) = 233,569

Residual sum of squares (RSS) = 14,054,602

Total sum of squares (TSS) = 14,288,171

TSS = RSS + MSS

<br>

#### 4c

What’s the R-square of this model?

**Answer:**

```{r Q4c}
summary(lm_Q2)$r.squared
```

<br>

#### 4d

What’s the correlation between lung capacity and age? 

**Verify that the R-square is the correlation squared.**

**Answer:**

```{r Q4d}
# Report the correlation between lung capacity and age
cor(dat$LungCapacitycc, dat$Age)
# Calculate squared correlation
cor(dat$LungCapacitycc, dat$Age)^2
# Report R-squared
summary(lm_Q2)$r.squared
# Examine if the squared correlation is equal to r-squared
round(cor(dat$LungCapacitycc, dat$Age)^2) == round(summary(lm_Q2)$r.squared)
```

<br>

### Question 5

Based on this model, how much difference do we expect to see between 20 years old and 25 year old? And between 60 years old and 65 years old?

**Hint:** By the nature of linear relationships, it doesn’t matter at what age ranges. This Model assumes that 1 year age difference leads to the same change in lung capacity.

**Answer:**

To solve this, we can directly use the property of a line. 

The **slope** means that when the independent variable, `X`, increases 1 unit, the dependent variable, `Y`, will increase $\beta_1$ unit. It doesn't matter whether `X` changes from 20 to 21, or 50 to 51.

This also means that a 5-unit increase in `X`, say from 20 to 25, or from 50 to 55, leads to $5 \times \beta_1$ unit change in `Y`.

The average change in `lung capacity` associated with a 5-year increase in `age` is: 

```{r Q5}
5 * coefficients(lm_Q2)["Age"]
```

Alternatively, we can calculate the fitted values at different Ages then compared the differences.

```{r Q5-alt}
# Create new dataset and store independent variables for prediction
new_dat <- data.frame(Age = c(20, 25, 60, 65))
# Predict lung capacity using the new dataset
predict(lm_Q2, newdata = new_dat)
# Difference between 20 and 25 years old
predict(lm_Q2, newdata = new_dat)[2] - predict(lm_Q2, newdata = new_dat)[1]
# Difference between 60 and 65 years old
predict(lm_Q2, newdata = new_dat)[4] - predict(lm_Q2, newdata = new_dat)[3]
```

<br>

Another interesting question:

Can you prescribe a 95% confidence interval for the average difference in `lung capacity` associated with a 5-year age difference? 

**Hint:** 
  $\beta_1$ is the difference due to 1-year difference in age.
	$5 * \beta_1$ is the difference due to a 5-years difference in age. 
		
**Answer:**

```{r Q5-2}
# Calculate: slope * 5
# Select the slope from the model summary
summary(lm_Q2)$coefficients[2, "Estimate"] * 5
```

<br>

Now the standard error in $\hat{\beta_1}$ is ___?

**Answer:**

```{r Q5-3}
# Get the standard error for the slope from the model summary
# Note: this is a different way to pull out the slope coefficient from 
# regression results
summary(lm_Q2)$coefficients[2, "Std. Error"]

```

What’s the standard error in $5 \times \hat\beta_1$?

**Answer:**

It is $5 \times S.E.$ of the slope coefficient estimate.

$$
\begin{align}
SE(5 \times \hat\beta_1) & = \sqrt{Var(5 \times \hat\beta_1)} \\
& = \sqrt{25 \times Var(\hat\beta_1}) \\
& = 5 \sqrt{Var(\hat\beta_1)} \\
& = 5 \times SE(\hat\beta_1)
\end{align}
$$

```{r Q5-4}
# Get the standard error for 5 * slope
summary(lm_Q2)$coefficients[2, "Std. Error"] * 5
```

```{r Q5-5}
# Calculate the confidence interval for 5 * slope
slope_Q5 <- summary(lm_Q2)$coefficients[2, "Estimate"] * 5
se_Q5 <- summary(lm_Q2)$coefficients[2, "Std. Error"] * 5
slope_Q5 - 2 * se_Q5
slope_Q5 + 2 * se_Q5

# Validate with: 5 * CI(beta_1)
5 * confint(lm_Q2)["Age", ]
```

<br>

### Question 6

Estimate standardized regression coefficients using two different approaches.

#### 6a

Generate a new variable based on lung capacity by dividing its original value by its standard deviation.

**Answer:**

```{r Q6a}
# Calculate standardized lung capacity and store the result as a new column
# in the original dataset
dat$std_lungCap <- dat$LungCapacitycc / sd(dat$LungCapacitycc)
```

<br>

Generate a new variable based on age by dividing its original value by its standard deviation. 
Run a regression using these two new variables. Report the regression coefficients (intercept and slope)

**Answer:**

```{r Q6a-2}
# Calculate standardized age and store the result in `dat`
dat$std_age <- dat$Age / sd(dat$Age)

# Fit a new regression model
lm_Q6 <- lm(std_lungCap ~ std_age, data = dat)
# Report coefficients
lm_Q6$coefficients
```

<br>

#### 6b

Use lm.beta to estimate the standardized regression coefficients.

**Answer:**

```{r Q6b}
summary(lm.beta(lm_Q6))
```

<br>

#### 6c

Compare standardized slope with correlation.

**Answer:**

```{r Q6c}
# Report standardized slope
lm_Q6$coefficients["std_age"]
# Report correlation between lung capacity and age
cor(dat$LungCapacitycc, dat$Age)
```

Standardized slope is equal to correlation.

<br>

#### 6d

What is the meaning of the intercept now?

**Answer:**

**Note that we applied different equations in these two approaches: **

For the first approach, we get a standardized (but not centered) variable by dividing original values with their standard deviation:

$$
\frac{X}{sd(X)}
$$

Now if we fit a new regression using standardized variables, we’ll get: 

- intercept = 11.972
- slope = 0.128

This regression model uses standard deviation as its measurement unit. The average of standardized lung capacity is 12.420. The average of standardized age is 3.506. So, if we take the mean value of standardized age as independent variable and put it in the model, we get: 

$$
0.128 * 3.506 = 0.448
$$

Adding the intercept, we get: 

$$
0.448 + 11.972 = 12.420
$$

, which is the mean value of the standardized lung capacity. A non-zero intercept indicates that our standardized regression model is not centered (shifting up if intercept is positive; down if negative).

For the second approach, we calculate a centered and standardized variable by first subtracting original values with their mean value:

$$
\frac{X - \overline{X}}{sd(X)}
$$

Now, since our scaling process involves both standardization and centering, we get an intercept that is zero. 

<br>

### Question 7

Regression Diagnostics

**Hint:** There are (at least) two R packages you can use, the `stat` package (more traditional, `?influence.measures`) and the `car` package that provides more stylized plots.

#### 7a

What's the difference between standardized residuals and studentized residuals? 

**Answer:**

The relationship between **standardized** residual and **studentized** residual is:

$$
\begin{align}
r_i & = Y_i - \hat{Y}_i \\
s_i & = \frac{r_i}{\hat{\sigma}} \\
t_i & = \frac{r_i}{\hat{\sigma}_{(-i)}}
\end{align}
$$

where $r_i$ is the $i^{th}$ residual, $\hat{\sigma}$ is the standard deviation of the residuals, and $\hat{\sigma}_{-i}$ is the standard deviation of the residuals calculated without the $i^{th}$ point. $s_i$ is the standardized residual and $t_i$ is the studentized residual. 

```{r Q7a}
# Calculate standardized residuals and store the result as `stand_res`
## Option 1:
stand_res <- residuals(lm_Q2) / sd(residuals(lm_Q2))
## Option 2:
stand_res <- rstandard(lm_Q2)

# Calculate studentized residuals
## Option 1: using the built-in R function
stu_residual <- rstudent(lm_Q2)
## Option 2: using for loop
# Create an empty vector to store results
stu_residual2 <- rep(NA, each = nrow(dat))
# FOR i iterating from 1 to the number of rows (n = 80):
for (i in 1:nrow(dat)) {
  stu_residual2[i] <- residuals(lm_Q2)[i] / sd(residuals(lm_Q2)[-i])
}

# Note: 
# 1. There are other versions of functions that can calculate studentized
# residuals. 
# 2. The essence is to leave one out in the calculation. 

# Visualize the differentce between studentized residuals and standardized 
# residuals
plot(x = stu_residual, y = stand_res, 
     pch = 19,                 # Point Character Style: 19
     xlab = "studentized",     # Label for the x-axis
     ylab = "standardized",    # Label for the y-axis
     cex = 0.5,                # Zoom out to 50%
     col = "red")              # Set color as red
abline(a = 0, b = 1)           # Draw a reference line
                               # Intercept = 0
                               # Slop = 1
```

Examine two figures: residuals versus $\hat{Y}$ and residuals versus Age.

**Answer:**

These two figures are almost identical, except the x axis for one is $\hat{Y}$ and `Age` for the other one. (In multiple regression, these two figures will be different, as other variables will affect $\hat{Y}$ as well.)

```{r Q7a-i-2}
# Create a 1-by-2 grid
par(mfrow = c(1, 2))
# Plot residuals against fitted lung capacity
plot(x = predict(lm_Q2), y = residuals(lm_Q2))
# Plot residuals against age
plot(x = dat$Age, y = residuals(lm_Q2))
```

The difference between the residual plot versus `Y` and the residual plot versus `Age` is the variable on the y-axis. The pattern looks essentially the same and the only difference is the scale of the axes. This is true for in simple regression but not true for multiple regression.

<br>

Any outliers? Use -2 and 2 as fences.

**Answer:**

```{r Q7a-ii}
# Detect outliers using studentized residuals.
# Store the index of studentized residuals that is greater than 2 or smaller 
# than -2.
index <- which(stu_residual > 2 | stu_residual < -2)
# Subset outliers from the original dataset using index
dat_outlier <- dat[index, ]
print(dat_outlier)


# Visualize outliers, using -2 and 2 as fences
plot(x = predict(lm_Q2), y = stu_residual)
abline(h = -2, col = "red")     # Draw a horizontal line at y = -2
abline(h =  2, col = "red")
```

<br>

##### 7a iii

Based on eyeballing, does linear relationship hold? 

**Answer:**

```{r Q7a-iii-2}
# Create a 1-by-2 grid
par(mfrow = c(1, 2))
# Plot Residuals vs. predictor (Age) using linear model: lm_Q2
plot(dat$Age, residuals(lm_Q2))
lines(lowess(residuals(lm_Q2) ~ dat$Age), col = "red") # Add a smooth trend line
abline(h = 0)

# Fit a new regression model with a new quadratic term
dat$Age2 <- dat$Age^2
lm_Q7 <- lm(LungCapacitycc ~ Age + Age2, data = dat)
# Plot Residuals vs. predictor (Age) using model: lm_Q7
plot(dat$Age, residuals(lm_Q7))
lines(lowess(residuals(lm_Q7) ~ dat$Age), col = "red")
abline(h = 0)
```

The red line is smoothed trend line. In general, when the linear relationship holds, we shouldn't expect to see any trend between residuals and the predictor. The left panel plots residuals versus `Age`, and the red line is the trend line. It doesn't seem like there is much of trend going up or down, or a particular quadratic shape (*we will get back to this point when we talk about polynomial regression.*) The right panel shows the residuals versus `Age` plot based on the model that the squared `Age` term is included. In this partiular example, we don't see much change. 

<br>

##### Question 7a iv

Based on eyeballing, does equal variance hold?

**Answer:**

The spread of residuals (along vertical axis) appears to be reasonably equal along the x-axis. **This suggests that equal variance is a reasonable assumption.**

```{r Q7a-iv-equalvar}
plot(x = predict(lm_Q2), y = residuals(lm_Q2))
```
<br>

Examine whether the normality assumption holds. 

**Answer:**

```{r Q7a-iv-normality}
# Create a 1-by-2 grid
par(mfrow = c(1, 2))
# Check normality using qq-plot
qqnorm(stand_res)
# Draw a 45-degree abstract reference line with intercept = 0, slope = 1
abline(a = 0, b = 1, lty = "dotted")
# Plot a histogram to examine the distribution of standardized residuals
hist(stand_res, 
     freq = FALSE,        # FALSE for probability density
     xlim = c(-3,3),      # x axis limits
     ylim = c(0,0.45),    # y axis limits
     main = "standardized resid versus normal dist")
lines(density(stand_res)) # Add a density reference line in black
# Add a normal reference line in red
lines(x = seq(from = -3, to = 3, by = 0.1), 
      y = dnorm(seq(from = -3, to = 3, by = 0.1), mean = 0, sd = 1), 
      col = "red")
```

**Left panel**: Q-Q plot of the residual terms allow us to examine the normality assumption. 

**Right panel**: histogram of the standardized residuals is compared with the normal distribution. 

In this case, the normality assumption doesn't hold very nicely, but, on the other hand, there is no obvious deviation (i.e. the distribution is still largely symmetric and unimodal).

<br>

Any observations have leverage, and high influence?

Eyeball by overlaying the fitted line - outliers along x and y axes?

**Answer:**

```{r Q7a-v}
# Reprint the plot
plot(x = predict(lm_Q2), y = stu_residual)
# Draw two fitted lines for reference
abline(h = mean(stu_residual), col = "red", lty = "dotted")
abline(v = mean(predict(lm_Q2)), col = "red", lty = "dotted")
```

<br>

Use a fancy plot.

**Answer:**

```{r Q7a-v-2}
# Plot a fancy influence plot
# Circle size is proportial to Cook's Distance
influencePlot(lm_Q2)
```

**Observations 65, 14 and 19 appear to have the largest Cook's Distance.**

<br>

Examine influence measures **Cook’s Distance** and **Dffits**.

**Answer:**

```{r Q7a-v-3}
# Option 1
summary(influence.measures(lm_Q2))

# Option 2: compute all measures
dat$cooksD <- cooks.distance(lm_Q2)                    # Cook's distance
dat$dffits <- dffits(lm_Q2)                            # Diffits
dat$leverage <- hatvalues(lm_Q2)                       # Find large hat values
# Detect influential points
influenceID <- dat$ID[dat$cooksD > 4/nrow(dat)]        # Threshold: 4/n
dffitsID <- dat$ID[dat$dffits>2 / (sqrt(2/nrow(dat)))] # Threshold: 2/sqrt(2/n)
dat$stand_res <- stand_res
# Detect outliers
outlierID <- dat$ID[abs(dat$stand_res) > 2]            # Greater than two-sigma
# Create a vector to store the index of influential points
star.points <- union(influenceID, dffitsID)
star.points <- union(star.points, outlierID)
print(dat[star.points, ])
```

<br>

##### 7a vi

Remove outlier data points, what happens? Things to check:
  - p-value of $\beta_1$, difference in values of new and old $\beta_1$ estimates.
  - SE values of new and old $\beta_1$ estimates.
  - R squares of the new and old models.
  
**Answer:**

```{r Q7a-vi}
# Remove outliers from the original dataset using index
dat_no_outlier <- dat[-outlierID, ]
# Fit a new regression model
lm_Q7_no_out <- lm(LungCapacitycc ~ Age, data = dat_no_outlier)
# Get the model summary
summary(lm_Q7_no_out)
```

The p-value of $\beta_1$ does not change much. The old p-value is 0.258 while the new p-value is 0.221.

The old standard error is 3.532. The new statistic is 3.241. The standard error decreases, indicating that the model has better performance in estimation.

The old r-squared is 0.01635. The new one is 0.1995. The r-squared increases, indicating that more variance can be explained by the new model.

<br>

Remove influential data points, what happens?

**Answer:**

```{r Q7a-vii}
# Update index to store influential points pulled from influentialPlot
# Remove influential points from the original dataset
dat_no_influ <- dat[-influenceID, ]
# Fit a new regression model
lm_Q7_no_influ <- lm(LungCapacitycc ~ Age, data = dat_no_influ)
# Get the model summary
summary(lm_Q7_no_influ)
```

After excluding influential points, the model continues to perform better.

<br>

### 7b

Notice that, in the dataset, we didn't really have high influential points nor high leverage points. 

#### 7b i

For the curious minds, if we remove data points that have high leverage but not an outlier or influential, what will happen? Can you fabricate some points to test the ideas?

**Hint:** you can add a new observation with a pair of lung capacity and age values of your choice and attach this new row to the existing data.

**Answer:**

```{r Q7b-1}
# Replot the scatterplot with wider x and y axes ranges
plot(LungCapacitycc ~ Age, data = dat, 
     xlim = c(0, 80), 
     ylim = c(1000, 6500))
abline(a = coef(lm_Q2)[1], b = coef(lm_Q2)[2], lty = "dotted") # Original regression line
# Manually drop a few data points
points(x = 10, y = 3000, pch = 16, col = "blue")     # Influential
points(x = 3 , y = 1200, pch = 16, col = "red")      # Leverage
points(x = 50, y = 1500, pch = 16, col = "green")    # Outlier

# Duplicate the dataset
datnew <-dat
datnew <- rbind(datnew, rep(NA, ncol(datnew))) # Add a several new rows

# Red point
# Age: 3
# Lung capacity: 1200
datnew$Age[81] <- 3
datnew$LungCapacitycc[81] <- 1200
# Fit a new regression model
lm_Q_p1 <- lm(LungCapacitycc ~ Age, dat = datnew)
abline(a = coef(lm_Q_p1)[1], b = coef(lm_Q_p1)[2], col = "red") # Regression line

# Blue point
# Age: 10
# Lung capacity: 3000
datnew$Age[81] <- 10
datnew$LungCapacitycc[81] <- 3000
# Fit a new regression model
lm_Q_p2 <- lm(LungCapacitycc ~ Age, dat = datnew)
abline(a = coef(lm_Q_p2)[1], b = coef(lm_Q_p2)[2], col = "blue") # Reg line

# Green point
# Age: 50
# Lung capacity: 1500
datnew$Age[81] <- 50
datnew$LungCapacitycc[81] <- 1500
# Fit a new regression model
lm_Q_p3 <- lm(LungCapacitycc ~ Age, dat = datnew)
abline(a = coef(lm_Q_p3)[1], b = coef(lm_Q_p3)[2], col = "green") # Reg line
```

The above plot shows the effect of three outliers to the regression model. The blue dot is a leverage point (along x-axis but close to the regression line). The red one is an influential point (along both x and y). The green one is simply an outlier (along y but not along x). The dotted line is the original regression line (without outliers). The color of each new regression line matches to the color of outliers. Results indicate that the red point has the largest influence to the model, followed by the blue point, then the green point. 

```{r Q7b-red}
# Create three vectors to store distance results
outlier_red <- c(cooks.distance(lm_Q_p1)[81], 
                 hatvalues(lm_Q_p1)[81],
                 rstandard(lm_Q_p1)[81])
outlier_blue <- c(cooks.distance(lm_Q_p2)[81], 
                 hatvalues(lm_Q_p2)[81],
                 rstandard(lm_Q_p2)[81])
outlier_green <- c(cooks.distance(lm_Q_p3)[81], 
                 hatvalues(lm_Q_p3)[81],
                 rstandard(lm_Q_p3)[81])
# Create a data frame to store vectors
outlier_distance <- data.frame(
  "Red" = outlier_red,
  "Blue" = outlier_blue,
  "Green" = outlier_green
)
# Add row labels
rownames(outlier_distance) <- c(
  "Cook's Distance",
  "Estimated Value",
  "Standardized Residual"
)
# Print the data frame
outlier_distance
```

From the above table, we see that the red point has the largest Cook's distance. Although the green point has a large standardized residual, it casts only marginal effects to the model. 

#### 7b ii

Lastly, think about when should we remove point(s) and when should we not remove point(s) from regression? What are the pros and cons?  

**Answer:**

When dealing with model fit, we really need to think about the bias-variance tradeoff. Model bias is determined by how well can the model explain the variation of the data. Variance is about how well can the model make predictions that reveal the true variation of the data. Ideally, we want to minimize bias and variance. However, in reality, it’s often difficult to achieve. Increasing the complexity will cause overfitting. Increasing sample size will alter model fit. Therefore, it’s hard to get to a perfect balance between bias and variance. When it comes to outliers, we need to do the same thing. Think and test: 

- Will the model bias increase too much if we remove an outlier? 
- Will I have more confidence with my model if I remove an outlier? 
- Is it the outlier that causes poor model fit? 

This [article](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229) is worth reading.
